{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.fitters.coxph_fitter import CoxPHFitter\n",
    "from sksurv.util import Surv\n",
    "from lifelines.utils import concordance_index\n",
    "from sksurv.metrics import integrated_brier_score\n",
    "from sksurv.metrics import cumulative_dynamic_auc\n",
    "from lifelines.statistics import proportional_hazard_test\n",
    "from lifelines import WeibullAFTFitter, ExponentialFitter, LogNormalAFTFitter, LogLogisticAFTFitter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis, ComponentwiseGradientBoostingSurvivalAnalysis\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# from bart_survival import surv_bart as sb\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(display=\"text\")\n",
    "\n",
    "print(pa.__version__)\n",
    "\n",
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "# for jupyter notebook, this is necessary to show plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the input directory for saving parquet files\n",
    "def read_file(file):\n",
    "    input_dir = '..'\n",
    "    path = f'{input_dir}/{file}.parquet'\n",
    "    file = pd.read_parquet(path)\n",
    "    return file\n",
    "\n",
    "# read files\n",
    "vitals = read_file('vitals')\n",
    "surgery = read_file('surgery')\n",
    "reanimatie = read_file('reanimatie')\n",
    "lab = read_file('lab')\n",
    "ic = read_file('ic_opnames')\n",
    "demo = read_file('demographics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reanimatie[\"care_order1\"] = reanimatie[\"care_order\"].astype(\"category\")\n",
    "print(reanimatie[\"care_order1\"].cat.categories)\n",
    "reanimatie[\"care_order1\"] = reanimatie[\"care_order1\"].cat.codes.astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop extra boolean columns. this is due to values are too skewed, and takes too much memory\n",
    "# but should keep prefix1 pattern\n",
    "def drop_bool_cols(df, prefixes):\n",
    "    cols_to_drop = [\n",
    "        col for col in df.columns\n",
    "        if any(col.startswith(prefix) and not col[len(prefix):].isdigit() for prefix in prefixes)\n",
    "    ]\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is too skewed with 'ic', and doesn't say much others\n",
    "ic = drop_bool_cols(ic, ['ic_specialisme_code', 'afdelings_code'])\n",
    "\n",
    "# this is also not representative, Klinische opname \n",
    "demo = drop_bool_cols(demo, ['opname_type_oms'])\n",
    "\n",
    "# 2nd useful. but memory issue. will keep the hoofdverrichting_code1\n",
    "surgery = drop_bool_cols(surgery, ['hoofdverrichting_code', 'prioriteit_code'])\n",
    "\n",
    "# useful. same\n",
    "demo = drop_bool_cols(demo, ['specialisme_code'])\n",
    "\n",
    "reanimatie = drop_bool_cols(reanimatie, ['care_order'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename categorical vars\n",
    "demo = demo.rename(columns={'specialisme_code1': 'specialisme_code'})\n",
    "surgery = surgery.rename(columns={'hoofdverrichting_code1': 'hoofdverrichting_code', 'prioriteit1': 'prioriteit_code'})\n",
    "surgery = surgery.drop(columns=[\"prioriteit\"])\n",
    "reanimatie = reanimatie.rename(columns={'care_order1': 'care_order'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo[\"geslacht\"] = demo[\"geslacht\"].astype(\"category\")\n",
    "print(demo[\"geslacht\"].cat.categories)\n",
    "demo[\"geslacht\"] = demo[\"geslacht\"].cat.codes.astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo[\"spoed\"] = demo[\"spoed\"].astype(\"category\")\n",
    "print(demo[\"spoed\"].cat.categories)\n",
    "demo[\"spoed\"] = demo[\"spoed\"].cat.codes.astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove boolean values because they take more memory\n",
    "demo = demo.drop(['geslacht_m', 'spoed_j'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # execute this code if there's a convergence issue with a model\n",
    "# #     if death_fullcode_first:\n",
    "# #        1\n",
    "# #     elif death_ic_first:\n",
    "# #         2\n",
    "# #     elif ic_6hr_first:\n",
    "# #         3\n",
    "# #     elif acute_ic_first:\n",
    "# #         4\n",
    "# demo['first_event'] = demo['first_event'].replace({4: 1, 2: 1})\n",
    "# demo['first_event'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo['count_combined'] = demo['count_death_fullcode'] | demo['count_death_ic'] | demo['count_acute_ic']\n",
    "# demo = demo.drop(['count_death_fullcode', 'count_death_ic', 'count_acute_ic'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # maybe use count instead\n",
    "# # necessary to avoid convergence issue\n",
    "# # ic_6hr\n",
    "# demo['is_combined'] = demo['death_fullcode'] | demo['death_ic'] | demo['acute_ic']\n",
    "# demo = demo.drop(['death_fullcode', 'death_ic', 'acute_ic'], axis=1)\n",
    "# demo['is_combined'] = demo['is_combined'].fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo['is_combined'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Selecting Subset of Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# why there are even duplicated rows in demo? no clue\n",
    "demo[demo.duplicated(keep=False)]\n",
    "demo = demo.drop_duplicates()\n",
    "ic = ic.drop_duplicates()\n",
    "surgery = surgery.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo['is_first'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_keys = demo.groupby('first_event').groups.keys()\n",
    "group_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.groupby('first_event').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo['first_event'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # stratified sampling: proportionate sampling with fixed seed\n",
    "# # if under 0.01, there are way less samples for some cases\n",
    "# demo = demo.groupby('first_event', group_keys=True).apply(lambda x: x.sample(frac=0.01, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frac 0.001 to Cox.\n",
    "# 0.01 (?) works with merging data but doesn't fit in Cox\n",
    "# just in case to have unique opname id per group, which will be expanded while merging\n",
    "def get_sample(group, frac=0.001, min_num=3, random_state=1):\n",
    "    group_unique = group.drop_duplicates(subset=\"opname_id\")\n",
    "    \n",
    "    # get sample size\n",
    "    n = math.ceil(len(group_unique) * frac)\n",
    "    \n",
    "    # ensures minimum numbers\n",
    "    n = max(n, min_num)\n",
    "    \n",
    "    # ensures not exceeding max number\n",
    "    n = min(n, len(group_unique))\n",
    "    \n",
    "    return group_unique.sample(n=n, random_state=random_state)\n",
    "\n",
    "demo = demo.groupby('first_event', group_keys=True).apply(get_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo[demo['first_event']==4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo['first_event'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo['first_event'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo['opname_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.iloc[0]['p_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo['is_first'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_drop = [\n",
    "    'death_fullcode', 'death_ic', 'acute_ic', 'ic_6hr', \n",
    "    'care_order_full_code', \n",
    "    'ic_ontslag_datum_tijd', 'ic_opname_datum_tijd', \n",
    "    'ok_eind_datum_tijd', 'prioriteit_code_acute', 'death_fullcode_first', \n",
    "    'death_ic_first', 'ic_6hr_first', 'acute_ic_first', 'overlijdens_datum', \n",
    "    #'count_death_fullcode', 'count_death_ic', 'count_ic_6hr', 'count_acute_ic', \n",
    "    #'first_event',\n",
    "    #'opname_datum_tijd', 'ontslag_datum_tijd'\n",
    "]\n",
    "\n",
    "demo = demo.drop(columns=columns_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a datetime variable from separate integer variables due to bug of parquet\n",
    "def create_datetime(df, original_col):\n",
    "    df = df.drop(original_col, axis=1)\n",
    "    \n",
    "    year_col = f\"{original_col}_year\"\n",
    "    month_col = f\"{original_col}_month\"\n",
    "    day_col = f\"{original_col}_day_of_month\"\n",
    "    hour_col = f\"{original_col}_hour\"\n",
    "    \n",
    "    col_map = {\n",
    "        year_col: 'year',\n",
    "        month_col: 'month',\n",
    "        day_col: 'day',\n",
    "        hour_col: 'hour'\n",
    "    }\n",
    "    \n",
    "    df = df[list(col_map.keys())].rename(columns=col_map)\n",
    "    return pd.to_datetime(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo['opname_datum_tijd'] = create_datetime(demo, 'opname_datum_tijd')\n",
    "demo['ontslag_datum_tijd'] = create_datetime(demo, 'ontslag_datum_tijd')\n",
    "\n",
    "surgery['ok_begin_datum_tijd'] = create_datetime(surgery, 'ok_begin_datum_tijd')\n",
    "surgery['ok_eind_datum_tijd'] = create_datetime(surgery, 'ok_eind_datum_tijd')\n",
    "\n",
    "reanimatie['vanaf_datum'] = create_datetime(reanimatie, 'vanaf_datum')\n",
    "\n",
    "lab['lab_datum_tijd'] = create_datetime(lab, 'lab_datum_tijd')\n",
    "\n",
    "ic['ic_opname_datum_tijd'] = create_datetime(ic, 'ic_opname_datum_tijd')\n",
    "ic['ic_ontslag_datum_tijd'] = create_datetime(ic, 'ic_ontslag_datum_tijd')\n",
    "\n",
    "vitals['meting_datum_tijd'] = create_datetime(vitals, 'meting_datum_tijd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling where event == 0 as stay time and convert all in hour unit\n",
    "time_diff = demo['ontslag_datum_tijd'] - demo['opname_datum_tijd']\n",
    "time_diff = (time_diff.dt.total_seconds() / 60).round().astype('UInt32')\n",
    "demo.loc[demo['first_event'] == 0, 'time_to_first_event'] = time_diff\n",
    "demo['time_to_first_event'] = (demo['time_to_first_event'] / 60).round().astype('UInt16')\n",
    "\n",
    "# I think we don't do anything with this period, as it is indicated in time_to_first_event\n",
    "demo = demo.drop(['opname_datum_tijd', 'ontslag_datum_tijd'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same reason\n",
    "dataframes = [demo, surgery, reanimatie, ic, lab]\n",
    "\n",
    "def drop_time_cols(dataframes):\n",
    "    suffixes = ['_year', '_month', '_day_of_month', '_hour']\n",
    "    for df in dataframes:\n",
    "        cols_drop = [col for col in df.columns if any(col.endswith(suffix) for suffix in suffixes)]\n",
    "        df.drop(columns=cols_drop, inplace=True)\n",
    "    \n",
    "drop_time_cols(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals = vitals.drop(['meting_datum_tijd_year',\n",
    "       'meting_datum_tijd_month', 'meting_datum_tijd_day_of_month',\n",
    "       'meting_datum_tijd_hour'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not necessary\n",
    "# def get_duration(df, start_col, end_col, new_col):\n",
    "#     df = df[df[start_col] <= df[end_col]].copy()\n",
    "#     time_diff = (df[end_col] - df[start_col])\n",
    "#     df[new_col] = (time_diff.dt.total_seconds() / 3600).round()\n",
    "#     df[new_col] = df[new_col].astype('int8')\n",
    "#     df.drop([start_col, end_col], axis=1, inplace=True)\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# surgery = get_duration(surgery, 'ok_begin_datum_tijd', 'ok_eind_datum_tijd', 'surgery_duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# surgery['surgery_duration'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo['is_first'] = demo['is_first'].astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo['first_event'] = demo['first_event'].astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo = demo.drop(['first_event'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# huh, it cannot handle 'UInt' or 'Int' type\n",
    "demo['time_to_first_event'] = demo['time_to_first_event'].astype('int32')\n",
    "\n",
    "# demo = demo.set_index(['p_id', 'opname_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Cleaning some rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_p(df):\n",
    "    p = df.isna().mean() * 100\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_p(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_p(vitals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we adjust the threshold here, then i'll remove some boolean variables as a compensation for memory\n",
    "# and what if we train the datetime columns?\n",
    "def drop_high_missing_cols(df, t=50):\n",
    "    p = missing_p(df)\n",
    "    columns_to_drop = p[p > t].index\n",
    "    df_dropped = df.drop(columns=columns_to_drop)\n",
    "    return df_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals = drop_high_missing_cols(vitals)\n",
    "vitals.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = drop_high_missing_cols(lab)\n",
    "lab.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals = vitals.drop(['meting_datum_tijd'], axis=1)\n",
    "vitals.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surgery.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surgery = surgery.drop(['operatie_id', 'ok_begin_datum_tijd', 'ok_eind_datum_tijd'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reanimatie.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reanimatie = reanimatie.drop(['vanaf_datum'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic['ic_6hr'] = ic['ic_6hr'].astype('boolean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better to use the count. it's more admission specific\n",
    "ic = ic.drop(['ic_6hr'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic = ic.drop(['ic_opname_datum_tijd', 'ic_ontslag_datum_tijd'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = lab.drop(['lab_datum_tijd'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing with nominal values for other variables\n",
    "\n",
    "# ANIONGAP\tAniongap\t8\n",
    "# BICARB\tBicarbonate\t27\n",
    "# BPDIA\tDiastolic blood pressure\t70\n",
    "# BPSYS\tSystolic blood pressure\t110\n",
    "# GLUCOSE\tGlucose\t5.5\n",
    "# HEMAT\tHematocrit\t0.34\n",
    "# HRTRT\tHeart rate\t80\n",
    "# LACT\tLactate\t0\n",
    "# BUN\tBlood urea nitrogen\t3.5\n",
    "# CREAT\tCreatinine\t115\n",
    "# AGE\tAge\t17\n",
    "# SAT\tSaturation\t99\n",
    "# RSPRT\tRespiration rate\t12\n",
    "# SODIUM\tSodium\t140\n",
    "# TEMP (C)\tTemprature\t37\n",
    "# TROP\tTroponin\t0\n",
    "# WBC\tWhite blood cell count\t5\n",
    "# PH\tBlood Ph\t7.41\n",
    "# BIL\tBilirubin\t8.2\n",
    "# ALB\tAlbumin\t34\n",
    "# PAC\tPaCO2\t39\n",
    "# PAO\tPaO2\t82\n",
    "\n",
    "nominal_values = {\n",
    "    'kreatinine': 115,\n",
    "    'natrium': 140,\n",
    "    'ureum': 3.5,\n",
    "    'hr_meet_waarde1': 80,\n",
    "    'nibp_meet_waarde1': 110,\n",
    "    'nibp_meet_waarde2': 70,\n",
    "    'nibp_meet_waarde3': 83.33,\n",
    "    'resp_meet_waarde1': 12,\n",
    "    'spo2_meet_waarde1': 99,\n",
    "    'temp_meet_waarde1': 37\n",
    "}\n",
    "\n",
    "vitals = vitals.fillna(value=nominal_values)\n",
    "lab = lab.fillna(value=nominal_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dtypes (UInt, Int to int as we're removing any NaN values and they take way less memory usage)\n",
    "\n",
    "dataframes = [vitals, surgery, reanimatie, lab, ic, demo]\n",
    "\n",
    "dtype_mapping = {\n",
    "    'Int8': 'int8',\n",
    "    'Int16': 'int16',\n",
    "    'Int32': 'int32',\n",
    "    'Int64': 'int64',\n",
    "\n",
    "    'UInt8': 'int8',\n",
    "    'UInt16': 'int16',\n",
    "    'UInt2': 'int32',\n",
    "    'UInt64': 'int64',\n",
    "}\n",
    "\n",
    "for df in dataframes:\n",
    "    for col in df.columns:\n",
    "        current_dtype = str(df[col].dtype)\n",
    "        if current_dtype in dtype_mapping:\n",
    "            df[col] = df[col].astype(dtype_mapping[current_dtype])\n",
    "    print(\"===========\")\n",
    "    print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Merging other datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataframes(df1, df2):\n",
    "    merged_df = pd.merge(df1, df2, on='opname_id', how='left', suffixes=('_x', '_y'))\n",
    "    merged_df = merged_df.drop(columns=[col for col in merged_df.columns if col.endswith('_y')])\n",
    "    merged_df = merged_df.rename(columns=lambda x: x.replace('_x', ''))\n",
    "    \n",
    "    # if no values, then fill in -1\n",
    "    for col in [\"hoofdverrichting_code\", \"prioriteit_code\", \"care_order\", \"m_year\", \"m_month\", \"m_day\", \"m_hour\"]:\n",
    "        if col in merged_df.columns:\n",
    "            if col == \"m_year\":\n",
    "                merged_df[col] = merged_df[col].fillna(-1).astype(\"int16\")\n",
    "            else:\n",
    "                merged_df[col] = merged_df[col].fillna(-1).astype(\"int8\")\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merge_dataframes(demo, surgery)\n",
    "del demo, surgery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nothing's gonna be involved from here\n",
    "# merged_df = merge_dataframes(merged_df, ic)\n",
    "# del ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merge_dataframes(merged_df, reanimatie)\n",
    "del reanimatie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merge_dataframes(merged_df, lab)\n",
    "del lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merge_dataframes(merged_df, vitals)\n",
    "del vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not used as i removed all bool/boolean values\n",
    "def replace_false(df):\n",
    "    for column in df.columns:\n",
    "        if pd.api.types.is_bool_dtype(df[column]):\n",
    "            df[column] = df[column].fillna(False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = replace_false(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bool_columns = [\n",
    "#     \"prioriteit_code_acute\",\n",
    "#     \"prioriteit_code_elective\",\n",
    "#     \"prioriteit_code_unknown\",\n",
    "#     \"care_order_dnr\",\n",
    "#     \"care_order_full_code\",\n",
    "#     \"care_order_partial\"\n",
    "# ]\n",
    "# df[bool_columns] = df[bool_columns].astype(bool)  \n",
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing with nominal values for other variables\n",
    "\n",
    "# ANIONGAP\tAniongap\t8\n",
    "# BICARB\tBicarbonate\t27\n",
    "# BPDIA\tDiastolic blood pressure\t70\n",
    "# BPSYS\tSystolic blood pressure\t110\n",
    "# GLUCOSE\tGlucose\t5.5\n",
    "# HEMAT\tHematocrit\t0.34\n",
    "# HRTRT\tHeart rate\t80\n",
    "# LACT\tLactate\t0\n",
    "# BUN\tBlood urea nitrogen\t3.5\n",
    "# CREAT\tCreatinine\t115\n",
    "# AGE\tAge\t17\n",
    "# SAT\tSaturation\t99\n",
    "# RSPRT\tRespiration rate\t12\n",
    "# SODIUM\tSodium\t140\n",
    "# TEMP (C)\tTemprature\t37\n",
    "# TROP\tTroponin\t0\n",
    "# WBC\tWhite blood cell count\t5\n",
    "# PH\tBlood Ph\t7.41\n",
    "# BIL\tBilirubin\t8.2\n",
    "# ALB\tAlbumin\t34\n",
    "# PAC\tPaCO2\t39\n",
    "# PAO\tPaO2\t82\n",
    "\n",
    "nominal_values = {\n",
    "    'kreatinine': 115,\n",
    "    'natrium': 140,\n",
    "    'ureum': 3.5\n",
    "}\n",
    "\n",
    "df = df.fillna(value=nominal_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove where the measurement records are non existing\n",
    "df = df.dropna(subset=['nibp_meet_waarde1'])\n",
    "df = df[df['m_year'] != -1]\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_seq_items', None)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(verbose=True, max_cols=None, show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [df]\n",
    "\n",
    "# initialize titles\n",
    "titles = [\"df_for_sa_rand1\"]\n",
    "\n",
    "# specify the output directory for saving parquet files\n",
    "output_dir = '..'\n",
    "\n",
    "# create the folder if it does not exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# save each dataframe as a parquet file\n",
    "for df, title in zip(dataframes, titles):\n",
    "    output_path = os.path.join(output_dir, f\"{title}.parquet\")\n",
    "\n",
    "    # check if the file already exists\n",
    "    if not os.path.isfile(output_path):\n",
    "        try:\n",
    "            # save dataframe to parquet file\n",
    "            df.to_parquet(output_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save {title}: {e}.\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    input_dir = '..'\n",
    "    path = f'{input_dir}/{file}.parquet'\n",
    "    file = pd.read_parquet(path)\n",
    "    return file\n",
    "\n",
    "# read files\n",
    "df1 = read_file('df_for_sa_rand1')\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Cleaning Done HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The below cells are for running survival analysis method.    \n",
    "And cox was to check convergence issue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# THE BELOW CODE IS JUST TO CHECK CONVERGENCE ISSUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # should've changed the 'is_first' as int..already did above\n",
    "\n",
    "# extra_cols = ['opname_id', 'p_id']\n",
    "\n",
    "# df1 = df.drop(columns=extra_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should've changed the 'is_first' as int..already did above\n",
    "\n",
    "df = df.set_index(['p_id', 'opname_id'])\n",
    "df = df.reset_index(drop=True)\n",
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1['care_order_partial'].value_counts()\n",
    "# df1['prioriteit_code_unknown'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = df1.drop(['prioriteit_code_unknown', 'care_order_partial', 'first_event'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df1, test_size=0.2, random_state=42)\n",
    "\n",
    "duration_col = 'time_to_first_event'\n",
    "event_col = 'is_first'\n",
    "\n",
    "X = df1.drop([event_col, duration_col], axis=1)\n",
    "y = df1[[event_col, duration_col]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Cox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit cox and get the estimation of hourly survival probability\n",
    "cph = CoxPHFitter(alpha=0.05, penalizer=0.01)\n",
    "cph.fit(train, duration_col='time_to_first_event', event_col='is_first', batch_mode=True)\n",
    "\n",
    "# define it to the max range for all entries\n",
    "times = list(range(0, max(df1['time_to_first_event']) + 1))\n",
    "\n",
    "survival = cph.predict_survival_function(X_test, times=times)\n",
    "survival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If your objective is prediction, you can focus on the loss metric\n",
    "# cph.check_assumptions(df1, p_value_threshold=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['is_first'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NOT NECESSARY BECAUSE THIS ONE WILL HAVE 'AVG_SURV PROBA' Column\n",
    "\n",
    "# dataframes = [df1]\n",
    "\n",
    "# # initialize titles\n",
    "# titles = [\"cox\"]\n",
    "\n",
    "# # specify the output directory for saving parquet files\n",
    "# output_dir = '/..'\n",
    "\n",
    "# # create the folder if it does not exist\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# # save each dataframe as a parquet file\n",
    "# for df, title in zip(dataframes, titles):\n",
    "#     output_path = os.path.join(output_dir, f\"{title}.parquet\")\n",
    "\n",
    "#     # check if the file already exists\n",
    "#     if not os.path.isfile(output_path):\n",
    "#         try:\n",
    "#             # save dataframe to parquet file\n",
    "#             df.to_parquet(output_path)\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Failed to save {title}: {e}.\")\n",
    "#             continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
